{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taller_Pytorch_T4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Estudiante: Roy Chavarría Garita\n",
        "## Semana #4"
      ],
      "metadata": {
        "id": "UtakeBZ4m7jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicios\n",
        "-Replicar lo visto en el video https://youtu.be/CYtLtIsdeSg"
      ],
      "metadata": {
        "id": "b0hklq_hm_ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objetivos\n",
        "- Implementar modelo en pytirch usando regresión logistica capaz de conocer imágenes\n",
        "- Reconocer problemas de clasificación y de entrenamiento de un modelo\n",
        "- Ejecutar el guardado y cargado de un modelo"
      ],
      "metadata": {
        "id": "PNoVuKLjnZPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imagen a utilizar\n",
        "![](https://camo.githubusercontent.com/01c057a753e92a9bc70b8c45d62b295431851c09cffadf53106fc0aea7e2843f/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)\n"
      ],
      "metadata": {
        "id": "3buLZJOmoeg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install numpy matplotlib torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK_TRTfHnC-v",
        "outputId": "95b55bcc-e00a-48e8-e2bc-d3f7b038bbd9"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        " "
      ],
      "metadata": {
        "id": "YH0jwP7CnS1g"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se descarga el dataset, contiene 60mil imágenes solo para entrenar al modelo, aparte existe 10k imágenes adicionales para evaluar el modelo.\n"
      ],
      "metadata": {
        "id": "6l4CoEhbp5jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MNIST(root='data/', download=True)"
      ],
      "metadata": {
        "id": "Fol5OyuOpkNa"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bandera train para indicar si queremos conjunto de entrenamiento o evaluación\n",
        "test_dataset = MNIST(root='data/', train=False)\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbHRKyrGqR_Q",
        "outputId": "30adeee9-9e83-433e-fa28-8666f4828a5c"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo para mostrar par con input y target"
      ],
      "metadata": {
        "id": "vh-UuPS2qsN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]\n",
        "a,b = dataset[0]\n",
        "print(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9RsKukEqzry",
        "outputId": "bde26608-407e-4815-fe9d-93b493247526"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=L size=28x28 at 0x7F2D220B3110> 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Sp9MlGv_q9UI"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imagen, etiqueta = dataset[0]\n",
        "print(imagen,\"Etiqueta o número: \"+str(etiqueta))\n",
        "plt.imshow(imagen, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "GLbVd_9ArftX",
        "outputId": "3ea25149-5fd6-493c-806b-99f93bf0af46"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PIL.Image.Image image mode=L size=28x28 at 0x7F2D2223BE90> Etiqueta o número: 5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2d239b0210>"
            ]
          },
          "metadata": {},
          "execution_count": 198
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch no traba con imagenes por lo que debemos de pasarlas a tensores"
      ],
      "metadata": {
        "id": "s2VPHzXBuBew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms  as transforms"
      ],
      "metadata": {
        "id": "Yn4gBf8huGCW"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mnist dataset (imagenes y etiquetas)\n",
        "dataset = MNIST(root='data/',\n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())\n",
        "imagen , etiqueta = dataset[0]\n",
        "#print(imagen)"
      ],
      "metadata": {
        "id": "opPq4JdSu6v0"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definicion de conjunto de datos\n",
        "\n",
        "- Training: para entrenar al modelo\n",
        "- Validation: para evaluar modelo durante el entrenamiento.\n",
        "- Existe Test: comparar nuestro modelo con otro y ver que tan preciso es."
      ],
      "metadata": {
        "id": "goNEdfRevhYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "dataset = MNIST(root='data/',\n",
        "                train=True,\n",
        "                transform=transforms.ToTensor())\n",
        "train_dataset, validation_dataset = random_split(dataset, [50000, 10000])"
      ],
      "metadata": {
        "id": "Hwfg2MkuvkwZ"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creacion de dataloaders y datasetss"
      ],
      "metadata": {
        "id": "2LHjPhdGwfiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size)"
      ],
      "metadata": {
        "id": "uoSYshB4wifS"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definición de modelo\n",
        " Usamos la funcion nn.linear()\n",
        " Recibe cantidad de elementos de input al modelo y otro parámetro que indica la cantidad de elementos a predecir\n",
        "\n",
        " Como lo de manzanas y naranjas, el primer parámetro son la cantidad de características y el segundo parámetro es la cantidad de respuestas a esas características (naranjas,manzanas)"
      ],
      "metadata": {
        "id": "mNoxh87exHGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#Pixeles\n",
        "input_size = 1*28*28\n",
        "\n",
        "# Numeros del 0 al 9\n",
        "output_size = 10\n",
        "\n",
        "# MOdelo de regresión\n",
        "model = nn.Linear(input_size, output_size)"
      ],
      "metadata": {
        "id": "oaEZh_UWxxwi"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model retorna pesos y sesgos asignados automaticamente\n"
      ],
      "metadata": {
        "id": "9nFEnNQ_0fNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.weight.shape)\n",
        "model.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLbLj0Hh0nR6",
        "outputId": "4d21caed-2dad-45c1-8e7a-4d04d175ac41"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.0143,  0.0300, -0.0230,  ..., -0.0013, -0.0121,  0.0231],\n",
              "        [ 0.0199,  0.0093, -0.0353,  ...,  0.0203, -0.0031, -0.0314],\n",
              "        [ 0.0299,  0.0040,  0.0019,  ..., -0.0312,  0.0241,  0.0300],\n",
              "        ...,\n",
              "        [ 0.0201,  0.0110, -0.0277,  ..., -0.0018, -0.0035, -0.0236],\n",
              "        [ 0.0325,  0.0077, -0.0180,  ...,  0.0155, -0.0250,  0.0089],\n",
              "        [ 0.0092,  0.0316, -0.0181,  ..., -0.0292,  0.0220,  0.0213]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.bias.shape)\n",
        "model.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXieAUXf0xEq",
        "outputId": "5fdf3afc-3fd3-4f60-d4a2-6879370dfa8d"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([ 0.0275, -0.0270,  0.0298, -0.0092,  0.0131,  0.0216, -0.0023,  0.0139,\n",
              "         0.0315,  0.0161], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo espera un vector de 784 ya que el tamaño del input es de 1x28x28, pero la imagen es una matriz de 1x28x28 por esta razon se hace un reshape"
      ],
      "metadata": {
        "id": "GfNR93oc08mi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imagen, etiqueta = train_dataset[0]\n",
        "print(imagen.shape)\n",
        "imagen = imagen.reshape(-1,784)\n",
        "prediccion = model(imagen)\n",
        "prediccion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IxDW3Ar38mP",
        "outputId": "936f3600-da16-477e-fbbb-9e62716e5dcf"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2021, -0.2763,  0.3498, -0.1892,  0.2489, -0.0891, -0.0152,  0.0241,\n",
              "          0.0438, -0.0445]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aparte se puede hacer una clase propia que extiende de la clase nn.MOdule para poder modificar nuestro modlo a nuestro antojo y también poder hacer que se haga el reshape dentro"
      ],
      "metadata": {
        "id": "e8nb2KGo6WhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Linear = nn.Linear(input_size, output_size)\n",
        "  \n",
        "  def forward(self, imagen):\n",
        "    imagen = imagen.reshape(-1,784)\n",
        "    prediccion = self.Linear(imagen)\n",
        "    return prediccion\n",
        "model = Model()\n",
        "imagen.etiqueta = train_dataset[0]\n",
        "prediccion = model(imagen)\n",
        "print(prediccion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrN-x1oZ6V3Q",
        "outputId": "8bd9f8fb-dc3a-4e70-c176-1c0e8382533b"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0005, -0.1056, -0.2664, -0.0235,  0.0289,  0.0042,  0.3203,  0.1522,\n",
            "         -0.1198,  0.0003]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.Linear.weight.shape, model.Linear.bias.shape)\n",
        "list(model.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cgfyPnE7vPZ",
        "outputId": "525bfdaf-fbb4-4f82-c0df-b2e553b55b62"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 784]) torch.Size([10])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.0124, -0.0289,  0.0335,  ..., -0.0278, -0.0114,  0.0137],\n",
              "         [-0.0140, -0.0354, -0.0030,  ..., -0.0199, -0.0060,  0.0146],\n",
              "         [ 0.0147,  0.0223, -0.0260,  ...,  0.0228, -0.0132,  0.0008],\n",
              "         ...,\n",
              "         [-0.0117,  0.0295, -0.0047,  ...,  0.0279,  0.0294, -0.0291],\n",
              "         [-0.0216, -0.0050, -0.0134,  ...,  0.0226, -0.0325,  0.0357],\n",
              "         [ 0.0122, -0.0019,  0.0210,  ..., -0.0078,  0.0032,  0.0103]],\n",
              "        requires_grad=True), Parameter containing:\n",
              " tensor([-0.0296, -0.0123,  0.0026,  0.0038, -0.0293,  0.0258,  0.0060,  0.0237,\n",
              "          0.0143, -0.0117], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La prediccion es una lista de de 10 numeros, debemos hacer que estos numeros sean la probabilidad de que ese numero sea el numero de la imagen\n",
        "\n",
        "La funcion softmax convierte estos valores en probabilidades, para que nada mas tengamos que sacar el maximo valor que significaría el numero que es más probable que sea la imagen."
      ],
      "metadata": {
        "id": "gcFZwAc-72pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Se aplica sofmax a las salidas\n",
        "probabilidades = F.softmax(prediccion, dim=1)\n",
        "\n",
        "maxima_probabilidad, numero = torch.max(probabilidades, dim=1)\n",
        "print(numero)\n",
        "print(maxima_probabilidad)\n",
        "\n",
        "def predecir_imagen(predicciones):\n",
        "  probabilidades = F.softmax(predicciones, dim=1)\n",
        "  maxima_probabilidad, numero = torch.max(probabilidades, dim=1)\n",
        "  return numero.item()  #saco el numero ya que variable es un tesor\n",
        "print(predecir_imagen(prediccion))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGbnWRHt8Tk5",
        "outputId": "80e27e00-0f7e-4cac-86fe-213fe58ec437"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6])\n",
            "tensor([0.1363], grad_fn=<MaxBackward0>)\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se define funcion de loss y creamos metrica de evaluación\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "loss_function = F.cross_entropy\n",
        "\n",
        "for imagenes, etiqutas in train_dataloader:\n",
        "  predicciones = model(imagenes)\n",
        "  break\n",
        "\n",
        "loss = loss_function(predicciones, etiqutas)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7qBMqDWDsLr",
        "outputId": "db004284-072d-463a-b6ce-f95554dc7cf7"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.2947, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# recibe preddicciones y etiquetas 128 y devuelve el porcentaje de las predicciones que están buenas\n",
        "\n",
        "def accurancy(predicciones, etiquetas):\n",
        "  _, numeros_predecidos = torch.max(predicciones, dim=1)\n",
        "  return torch.tensor(torch.sum(numeros_predecidos == etiquetas).item() / len(numeros_predecidos))"
      ],
      "metadata": {
        "id": "lyY1PvgIEe_W"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamiento del modelo\n",
        "LO mismo que en videos anteriores, pero con un paso extra para validar el modelo con el conjunto de validación"
      ],
      "metadata": {
        "id": "VQYesA3UFE9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainig(model,epochs, learning_rate, train_dataloader, validation_dataloader):\n",
        "  optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    #Fase de entrenamiento\n",
        "    for batch in train_dataloader:\n",
        "\n",
        "      #genero predicciones\n",
        "      imagenes, etiquetas = batch\n",
        "      predicciones = model(imagenes)\n",
        "      loss = F.cross_entropy(predicciones, etiquetas)\n",
        "\n",
        "      #calculo de gradiente\n",
        "      loss.backward()\n",
        "\n",
        "      #ajusto pesos y sesgos\n",
        "      optimizer.step()\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "      imagenes, etiquetas = batch\n",
        "      predicciones = model(imagenes)\n",
        "      loss = F.cross_entropy(predicciones, etiquetas)\n",
        "      acc = accurancy(predicciones, etiquetas)\n",
        "\n",
        "    print(\"Loss: \"+str(loss)+\"Acurrancy: \"+str(acc))"
      ],
      "metadata": {
        "id": "-e4B88-1FDa7"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainig(model,5,0.05, train_dataloader, validation_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zV3XN2HGuj5",
        "outputId": "9557c5c5-1d2d-4621-a1e9-8b62c90607eb"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(1.8310, grad_fn=<NllLossBackward0>)Acurrancy: tensor(0.8125)\n",
            "Loss: tensor(4.3023, grad_fn=<NllLossBackward0>)Acurrancy: tensor(0.8125)\n",
            "Loss: tensor(3.6058, grad_fn=<NllLossBackward0>)Acurrancy: tensor(0.8125)\n",
            "Loss: tensor(5.6528, grad_fn=<NllLossBackward0>)Acurrancy: tensor(0.8125)\n",
            "Loss: tensor(1.1636, grad_fn=<NllLossBackward0>)Acurrancy: tensor(0.9375)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probar la prediccion de imagenes"
      ],
      "metadata": {
        "id": "PuEhmHemHJjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import test\n",
        "#defino dataset para leer imagenes\n",
        "test_dataset = MNIST(root='data/',\n",
        "                     train=False,\n",
        "                     transform=transforms.ToTensor())\n",
        "\n",
        "#agarro una de las imagenes con su etiqueta\n",
        "imagen, etiqueta = test_dataset[2]\n",
        "plt.imshow(imagen[0], cmap='gray')\n",
        "\n",
        "imagen = imagen.unsqueeze(0)\n",
        "predicciones = model(imagen)\n",
        "print(\"Etiquetas\", etiqueta, \"Imagen preecida por el modelo: \", predecir_imagen(predicciones))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "TikKtoJuHIiN",
        "outputId": "711c973b-dfe3-4910-c4f1-0880e26abd9d"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Etiquetas 1 Imagen preecida por el modelo:  1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}